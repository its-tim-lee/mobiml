{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu-bEo7PiPxx"
      },
      "source": [
        "# MobiML GeoTrackNet Demo\n",
        "\n",
        "Based on: https://github.com/CIA-Oceanix/GeoTrackNet (MIT Licensed, (c) 2018 Duong Nguyen)\n",
        "\n",
        "As presented in Nguyen, D., Vadaine, R., Hajduch, G., Garello, R. (2022). GeoTrackNet - A Maritime Anomaly Detector Using Probabilistic Neural Network Representation of AIS Tracks and A Contrario Detection. In IEEE Transactions on Intelligent Transportation Systems, 23(6).\n",
        "\n",
        "\n",
        "Using data from AISDK: http://web.ais.dk/aisdata/aisdk-2018-02.zip\n",
        "\n",
        "*It is possible to further explore maritime traffic patterns with the TrAISformer (https://github.com/CIA-Oceanix/TrAISformer), which is used for vessel trajectory prediction. The TrAISformer can be trained with AIS data and the preprocessing steps are similar to those of GeoTrackNet. However, the TrAISformer is out of the scope of MobiML and is an optional extension for the user to explore.*\n",
        "\n",
        "## Environments\n",
        "\n",
        "### Preprocessing\n",
        "\n",
        "It is recommended to perform the preprocessing steps with the MobiML environment.\n",
        "\n",
        "### Model Training\n",
        "\n",
        "Set up a dedicated GeoTrackNet environment (PY3GPU) to train the model as instructed by Nguyen et al. (2022)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i4ZufBkiTnm",
        "outputId": "deb2a397-d1ef-4ed2-f179-9eeb99ee2dc8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t\t       mobiml-fl-demo.ipynb\t   mobiml-sampling.ipynb\n",
            "environment-flwr.yml   mobiml-geotracknet.ipynb    mobiml-transforms.ipynb\n",
            "environment-viz.yml    mobiml-nautilus.ipynb\t   README.md\n",
            "mobiml-datasets.ipynb  mobiml-preprocessing.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 0. clone and enter examples ────────────────────────────────────────────────\n",
        "!git clone https://github.com/movingpandas/mobiml.git\n",
        "%cd mobiml/examples                      # puts notebook and ../src side-by-side"
      ],
      "metadata": {
        "id": "ls5V_vQQiXTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) System libs\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y libspatialindex-dev gdal-bin libgdal-dev proj-bin libproj-dev\n",
        "\n",
        "# 2) Python packages\n",
        "!pip install \\\n",
        "  numpy pandas geopandas shapely pygeos rtree fiona movingpandas tqdm matplotlib \\\n",
        "  'dm-sonnet<2' absl-py h5py stonesoup pymeos gcsfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY5O40yUjdYu",
        "outputId": "adc9f32e-9495-478b-98a9-2a71fba06436"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libspatialindex-dev is already the newest version (1.9.3-2).\n",
            "gdal-bin is already the newest version (3.8.4+dfsg-1~jammy0).\n",
            "libgdal-dev is already the newest version (3.8.4+dfsg-1~jammy0).\n",
            "libproj-dev is already the newest version (9.3.1-1~jammy0).\n",
            "proj-bin is already the newest version (9.3.1-1~jammy0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 67 not upgraded.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: pygeos in /usr/local/lib/python3.11/dist-packages (0.14)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Collecting fiona\n",
            "  Using cached fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "Requirement already satisfied: movingpandas in /usr/local/lib/python3.11/dist-packages (0.22.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: dm-sonnet<2 in /usr/local/lib/python3.11/dist-packages (1.36)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Collecting stonesoup\n",
            "  Downloading stonesoup-1.6-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pymeos\n",
            "  Downloading pymeos-1.2.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona) (2025.4.26)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona) (8.2.1)\n",
            "Collecting click-plugins>=1.0 (from fiona)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (from movingpandas) (2.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tensorflow-probability<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from dm-sonnet<2) (0.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from dm-sonnet<2) (1.17.0)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.11/dist-packages (from dm-sonnet<2) (2.10.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.11/dist-packages (from dm-sonnet<2) (21.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from dm-sonnet<2) (1.17.2)\n",
            "Collecting mergedeep (from stonesoup)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting ordered-set (from stonesoup)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: plotly>=5.0 in /usr/local/lib/python3.11/dist-packages (from stonesoup) (5.24.1)\n",
            "Collecting pymap3d (from stonesoup)\n",
            "  Downloading pymap3d-3.1.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting ruamel.yaml>=0.16.5 (from stonesoup)\n",
            "  Downloading ruamel.yaml-0.18.11-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from stonesoup) (1.15.3)\n",
            "Collecting utm (from stonesoup)\n",
            "  Downloading utm-0.8.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pymeos-cffi<2,>=1.2.0 (from pymeos)\n",
            "  Downloading pymeos_cffi-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.15)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.20.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0->stonesoup) (9.1.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from pymeos-cffi<2,>=1.2.0->pymeos) (1.17.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.16.5->stonesoup)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: cloudpickle==1.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability<0.9.0,>=0.8.0->dm-sonnet<2) (1.1.1)\n",
            "Requirement already satisfied: gast<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability<0.9.0,>=0.8.0->dm-sonnet<2) (0.2.2)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy->movingpandas) (2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->pymeos-cffi<2,>=1.2.0->pymeos) (2.22)\n",
            "Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stonesoup-1.6-py3-none-any.whl (429 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.6/429.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymeos-1.2.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading pymeos_cffi-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.11-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pymap3d-3.1.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utm-0.8.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: utm, ruamel.yaml.clib, pymap3d, ordered-set, mergedeep, cligj, click-plugins, ruamel.yaml, pymeos-cffi, fiona, stonesoup, pymeos\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.10.1 mergedeep-1.3.4 ordered-set-4.1.0 pymap3d-3.1.0 pymeos-1.2.0 pymeos-cffi-1.2.0 ruamel.yaml-0.18.11 ruamel.yaml.clib-0.2.12 stonesoup-1.6 utm-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mobiml/examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB-kLnwUibsh",
        "outputId": "2dfd6677-dca9-44ce-ca51-cdf75aacefe1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'mobiml/examples'\n",
            "/content/mobiml/examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ../examples/data\n",
        "!wget -qO ../examples/data/aisdk_20180208_sample.zip \\\n",
        "        https://raw.githubusercontent.com/movingpandas/mobiml/main/examples/data/aisdk_20180208_sample.zip\n",
        "!unzip -q ../examples/data/aisdk_20180208_sample.zip \\\n",
        "        -d ../examples/data/aisdk_20180208_sample\n"
      ],
      "metadata": {
        "id": "DY0f016AidKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63yEB-A_iPxy"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zGPrhPnsiPxy",
        "outputId": "bed10510-7daf-4fe8-9c96-8799d9ff3d3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "code() argument 13 must be str, not int",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ba4a7ebea1d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmovingpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     from pandas.compat import (\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mis_numpy_dev\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from pandas.compat.pyarrow import (\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpa_version_under10p1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpa_version_under11p0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/compat/pyarrow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0m_palv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0m_gc_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misenabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_gc_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/compat.pxi\u001b[0m in \u001b[0;36minit pyarrow.lib\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.1.1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    207\u001b[0m             )\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCodeType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"co_posonlyargcount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             return types.CodeType(\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_posonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Python3.8 with PEP570\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: code() argument 13 must be str, not int"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import movingpandas as mpd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "sys.path.append(\"../src\")\n",
        "from mobiml.datasets import AISDK\n",
        "from mobiml.samplers import TemporalSplitter\n",
        "from mobiml.preprocessing import (\n",
        "    TrajectorySplitter,\n",
        "    TrajectoryFilter,\n",
        "    TrajectoryDownsampler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CcY9tqciPxy"
      },
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNfCCsbKiPxz"
      },
      "outputs": [],
      "source": [
        "# AISDK dataset\n",
        "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
        "\n",
        "EPOCH = datetime(1970, 1, 1)\n",
        "\n",
        "SOG_MIN = 2.0\n",
        "SOG_MAX = 30.0  # SOG is truncated to 30.0 knots max\n",
        "\n",
        "# Pkl filenames\n",
        "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
        "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
        "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
        "\n",
        "# Path to csv files\n",
        "data_path = \"data/aisdk_20180208_sample/\"\n",
        "csv_filename = \"aisdk_20180208_sample.csv\"\n",
        "\n",
        "# Output path\n",
        "out_path = \"data/aisdk_20180208_sample/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSLmW8kQiPxz"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFddOr6liPxz"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(data_path, csv_filename)\n",
        "print(f\"{datetime.now()} Loading data from {path}\")\n",
        "aisdk = AISDK(path)  # you can specify a bounding box here to filter the area\n",
        "LON_MIN, LAT_MIN, LON_MAX, LAT_MAX = aisdk.get_bounds()\n",
        "print(\n",
        "    f\"Bounding box:\\nmin_lon: {LON_MIN}\\nmin_lat: {LAT_MIN}\\nmax_lon: {LON_MAX}\\nmax_lat: {LAT_MAX}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L6hgV__iPxz"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OxcVtYHiPxz"
      },
      "source": [
        "#### Remove missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InHAvSDPiPxz"
      },
      "outputs": [],
      "source": [
        "aisdk.df = aisdk.df.dropna()\n",
        "aisdk.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjrj4hwdiPxz"
      },
      "outputs": [],
      "source": [
        "print(\"After removing missing values we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])\n",
        "print(\"Total number of vessels:\", len(aisdk.df.traj_id.unique()))\n",
        "print(\"Lat min: \", aisdk.df.y.min(), \"Lat max: \", aisdk.df.y.max())\n",
        "print(\"Lon min: \", aisdk.df.x.min(), \"Lon max: \", aisdk.df.x.max())\n",
        "print(\"Time min: \", aisdk.df.timestamp.min(), \"Time max: \", aisdk.df.timestamp.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyAA4kDriPx0"
      },
      "source": [
        "#### Remove 'Moored' and 'At anchor' AIS messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFCuI5JsiPx0"
      },
      "outputs": [],
      "source": [
        "aisdk.df.drop(aisdk.df[(aisdk.df[\"nav_status\"] == \"Moored\") | (aisdk.df[\"nav_status\"] == \"At anchor\")].index, inplace=True)\n",
        "print(\"After removing 'Moored' or 'At anchor' AIS messages we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzD5bN46iPx0"
      },
      "source": [
        "#### Keep only 'Cargo', 'Tanker', 'Passenger' vessel types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE8yM0wziPx0"
      },
      "outputs": [],
      "source": [
        "aisdk.df = aisdk.df[\n",
        "    (aisdk.df[\"ship_type\"] == \"Cargo\")\n",
        "    | (aisdk.df[\"ship_type\"] == \"Tanker\")\n",
        "    | (aisdk.df[\"ship_type\"] == \"Passenger\")\n",
        "]\n",
        "print(\"After keeping only 'Cargo', 'Tanker' or 'Passenger' AIS messages we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnQlJFq3iPx0"
      },
      "source": [
        "#### Split trajectories with observation gaps > 2 hrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iOg4-saiPx0"
      },
      "outputs": [],
      "source": [
        "aisdk = TrajectorySplitter(aisdk).split(observation_gap=timedelta(hours=2))\n",
        "print(\"After splitting trajectories with observation gaps we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLu6CRWjiPx0"
      },
      "source": [
        "#### Drop trajectories with fewer than $Points_{min}$ locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygsxrh8PiPx0"
      },
      "outputs": [],
      "source": [
        "aisdk = TrajectoryFilter(aisdk).filter_min_pts(min_pts=20)\n",
        "print(\"After removing trajectories with too few points we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcdkweYwiPx0"
      },
      "source": [
        "#### Drop speed outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m5_xu8ViPx0"
      },
      "outputs": [],
      "source": [
        "aisdk = TrajectoryFilter(aisdk).filter_speed(min_speed=SOG_MIN, max_speed=SOG_MAX)\n",
        "print(\"After removing speed outliers by setting a minimum and maximum speed we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcGJIPBSiPx0"
      },
      "outputs": [],
      "source": [
        "tc = aisdk.to_trajs() #  mpd.TrajectoryCollection(aisdk.df, \"traj_id\", t=\"timestamp\", x=\"x\", y=\"y\")\n",
        "traj_gdf = tc.to_traj_gdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-83kJ5iiPx0"
      },
      "source": [
        "We may also want to remove trajectories based on their overall average speed rather than the SOG values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czlAdXK9iPx0"
      },
      "outputs": [],
      "source": [
        "for index, row in traj_gdf.iterrows():\n",
        "    traj_gdf.loc[index, \"speed_ok\"] = (\n",
        "        tc.trajectories[index].get_length()\n",
        "        / tc.trajectories[index].get_duration().total_seconds()\n",
        "        > 1.02889  # 2 knots\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH6jRba0iPx0"
      },
      "outputs": [],
      "source": [
        "traj_gdf = traj_gdf[traj_gdf[\"speed_ok\"] == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU0KaBfriPx1"
      },
      "outputs": [],
      "source": [
        "aisdk.df = pd.merge(aisdk.df, traj_gdf[\"traj_id\"], how=\"inner\")\n",
        "print(\"After removing speed outliers based on length and duration we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVJtxAPriPx1"
      },
      "source": [
        "### Training data preparation\n",
        "#### Subsample AIS tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grhH_cqUiPx1"
      },
      "outputs": [],
      "source": [
        "aisdk = TrajectoryDownsampler(aisdk).subsample(min_dt_sec=60)\n",
        "print(\"After subsampling AIS tracks we have...\")\n",
        "print(\"Total number of AIS messages: \", aisdk.df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Xk9CLLiPx1"
      },
      "source": [
        "#### Temporal train/valid/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neBH5dw0iPx1"
      },
      "outputs": [],
      "source": [
        "aisdk = TemporalSplitter(aisdk).split_hr()\n",
        "aisdk.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el6QA_GqiPx1"
      },
      "outputs": [],
      "source": [
        "aisdk_train = aisdk.df[(aisdk.df[\"split\"] == 1.0)]\n",
        "aisdk_valid = aisdk.df[(aisdk.df[\"split\"] == 2.0)]\n",
        "aisdk_test = aisdk.df[(aisdk.df[\"split\"] == 3.0)]\n",
        "\n",
        "print(\"Total number of AIS messages: \", len(aisdk.df))\n",
        "print(\"Number of msgs in the training set: \", len(aisdk_train))\n",
        "print(\"Number of msgs in the validation set: \", len(aisdk_valid))\n",
        "print(\"Number of msgs in the test set: \", len(aisdk_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6-tIUR9iPx1"
      },
      "outputs": [],
      "source": [
        "aisdk_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPeHArwfiPx1"
      },
      "outputs": [],
      "source": [
        "target_column_order=[\"y\", \"x\", \"speed\", \"direction\", \"Name\", \"ship_type\", \"nav_status\", \"timestamp\", \"traj_id\"]\n",
        "aisdk_train = aisdk_train[target_column_order].reset_index(drop=True)\n",
        "aisdk_valid = aisdk_valid[target_column_order].reset_index(drop=True)\n",
        "aisdk_test = aisdk_test[target_column_order].reset_index(drop=True)\n",
        "aisdk_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXcSOmSLiPx1"
      },
      "source": [
        "#### Format timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtT83WraiPx1"
      },
      "outputs": [],
      "source": [
        "aisdk_train[\"timestamp\"] = (aisdk_train[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
        "aisdk_valid[\"timestamp\"] = (aisdk_valid[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
        "aisdk_test[\"timestamp\"]  = (aisdk_test[\"timestamp\"].astype(int) / 1_000_000_000).astype(int)\n",
        "aisdk_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lsVhS4MiPx1"
      },
      "source": [
        "#### Format to ndarrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F97g8OmiPx1"
      },
      "outputs": [],
      "source": [
        "aisdk_train = np.array(aisdk_train)\n",
        "aisdk_valid = np.array(aisdk_valid)\n",
        "aisdk_test = np.array(aisdk_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvZMgfjYiPx1"
      },
      "source": [
        "#### Merging into dict\n",
        "Creating AIS tracks from the list of AIS messages. Each AIS track is formatted by a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJCC8AeEiPx1"
      },
      "outputs": [],
      "source": [
        "print(\"Convert to dicts of vessel's tracks...\")\n",
        "\n",
        "def convert_tracks_to_dicts(tracks):\n",
        "    d = dict()\n",
        "    for v_msg in tqdm(tracks):\n",
        "        mmsi = int(v_msg[TRAJ_ID])\n",
        "        if not (mmsi in list(d.keys())):\n",
        "            d[mmsi] = np.empty((0, 9))\n",
        "        d[mmsi] = np.concatenate(\n",
        "            (d[mmsi], np.expand_dims(v_msg[:9], 0)), axis=0\n",
        "        )\n",
        "    for key in tqdm(list(d.keys())):\n",
        "        d[key] = np.array(\n",
        "            sorted(d[key], key=lambda m_entry: m_entry[TIMESTAMP])\n",
        "        )\n",
        "    return d\n",
        "\n",
        "Vs_train = convert_tracks_to_dicts(aisdk_train)\n",
        "Vs_valid = convert_tracks_to_dicts(aisdk_valid)\n",
        "Vs_test = convert_tracks_to_dicts(aisdk_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJzjwlBiPx2"
      },
      "source": [
        "#### Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ByWIQLwiPx2"
      },
      "outputs": [],
      "source": [
        "print(\"Normalising data ...\")\n",
        "\n",
        "def normalize(d):\n",
        "    for k in tqdm(list(d.keys())):\n",
        "        v = d[k]\n",
        "        v[:, LAT] = (v[:, LAT] - LAT_MIN) / (LAT_MAX - LAT_MIN)\n",
        "        v[:, LON] = (v[:, LON] - LON_MIN) / (LON_MAX - LON_MIN)\n",
        "        v[:, SOG][v[:, SOG] > SOG_MAX] = SOG_MAX\n",
        "        v[:, SOG] = v[:, SOG] / SOG_MAX\n",
        "        v[:, COG] = v[:, COG] / 360.0\n",
        "    return d\n",
        "\n",
        "Vs_train = normalize(Vs_train)\n",
        "Vs_valid = normalize(Vs_valid)\n",
        "Vs_test = normalize(Vs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZieLZ_jiPx2"
      },
      "outputs": [],
      "source": [
        "for filename, filedict in zip(\n",
        "    [pkl_filename_train, pkl_filename_valid, pkl_filename_test],\n",
        "    [Vs_train, Vs_valid, Vs_test],\n",
        "):\n",
        "    print(\"Writing to\", os.path.join(out_path, filename))\n",
        "    with open(os.path.join(out_path, filename), \"wb\") as f:\n",
        "        pickle.dump(filedict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4q1rwQiiPx2"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "From this point forward, it is recommended to execute the code with the [PY3GPU environment](https://github.com/CIA-Oceanix/GeoTrackNet/blob/master/requirements.yml), as set up by Nguyen et al. (2022).\n",
        "\n",
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyQIIYq2iPx2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# AISDK dataset\n",
        "LAT, LON, SOG, COG, NAME, SHIPTYPE, NAV_STT, TIMESTAMP, TRAJ_ID = list(range(9))\n",
        "\n",
        "# Pkl filenames\n",
        "pkl_filename_train = \"aisdk_20180208_train.pkl\"\n",
        "pkl_filename_valid = \"aisdk_20180208_valid.pkl\"\n",
        "pkl_filename_test = \"aisdk_20180208_test.pkl\"\n",
        "\n",
        "data_path = \"../examples/data/aisdk_20180208_sample/\"\n",
        "dataset_path = os.path.join(data_path, pkl_filename_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPjqHeZYiPx2"
      },
      "source": [
        "### Calculate AIS mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjqYo158iPx2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "LAT_BINS = 100\n",
        "LON_BINS = 200\n",
        "SOG_BINS = 30\n",
        "COG_BINS = 72\n",
        "\n",
        "\n",
        "def sparse_AIS_to_dense(msgs_, num_timesteps, mmsis):\n",
        "    def create_dense_vect(msg, lat_bins=100, lon_bins=200, sog_bins=30, cog_bins=72):\n",
        "        lat, lon, sog, cog = msg[0], msg[1], msg[2], msg[3]\n",
        "        data_dim = lat_bins + lon_bins + sog_bins + cog_bins\n",
        "        dense_vect = np.zeros(data_dim)\n",
        "        dense_vect[int(lat * lat_bins)] = 1.0\n",
        "        dense_vect[int(lon * lon_bins) + lat_bins] = 1.0\n",
        "        dense_vect[int(sog * sog_bins) + lat_bins + lon_bins] = 1.0\n",
        "        dense_vect[int(cog * cog_bins) + lat_bins + lon_bins + sog_bins] = 1.0\n",
        "        return dense_vect\n",
        "\n",
        "    dense_msgs = []\n",
        "    for msg in msgs_:\n",
        "        dense_msgs.append(\n",
        "            create_dense_vect(\n",
        "                msg,\n",
        "                lat_bins=LAT_BINS,\n",
        "                lon_bins=LON_BINS,\n",
        "                sog_bins=SOG_BINS,\n",
        "                cog_bins=COG_BINS,\n",
        "            )\n",
        "        )\n",
        "    dense_msgs = np.array(dense_msgs)\n",
        "    return dense_msgs, num_timesteps, mmsis\n",
        "\n",
        "\n",
        "dirname = os.path.dirname(dataset_path)\n",
        "\n",
        "try:\n",
        "    with tf.io.gfile.GFile(dataset_path, \"rb\") as f:\n",
        "        Vs = pickle.load(f)\n",
        "except:\n",
        "    with tf.io.gfile.GFile(dataset_path, \"rb\") as f:\n",
        "        Vs = pickle.load(f, encoding=\"latin1\")\n",
        "\n",
        "data_dim = LAT_BINS + LON_BINS + SOG_BINS + COG_BINS\n",
        "\n",
        "mean_all = np.zeros((data_dim,))\n",
        "sum_all = np.zeros((data_dim,))\n",
        "total_ais_msg = 0\n",
        "\n",
        "current_mean = np.zeros((0, data_dim))\n",
        "current_ais_msg = 0\n",
        "\n",
        "count = 0\n",
        "for mmsi in list(Vs.keys()):\n",
        "    count += 1\n",
        "    print(count)\n",
        "    tmp = Vs[mmsi][:, [LAT, LON, SOG, COG]]\n",
        "    tmp[tmp == 1] = 0.99999\n",
        "    current_sparse_matrix, _, _ = sparse_AIS_to_dense(tmp, 0, 0)\n",
        "    #    current_mean = np.mean(current_sparse_matrix,axis = 0)\n",
        "    sum_all += np.sum(current_sparse_matrix, axis=0)\n",
        "    total_ais_msg += len(current_sparse_matrix)\n",
        "\n",
        "mean = sum_all / total_ais_msg\n",
        "\n",
        "print(\"Writing to\", os.path.join(dirname, \"/mean.pkl\"))\n",
        "with open(dirname + \"/mean.pkl\", \"wb\") as f:\n",
        "    pickle.dump(mean, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeXjlSqPiPx2"
      },
      "source": [
        "### Training\n",
        "\n",
        "#### Step 1: Training the Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqKwMUh1iPx2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "# import logging\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import mobiml.models.geotracknet.runners as runners\n",
        "from mobiml.models.geotracknet.flags_config import config\n",
        "\n",
        "print(config.trainingset_path)\n",
        "fh = logging.FileHandler(os.path.join(config.logdir, config.log_filename + \".log\"))\n",
        "# tf.logging.set_verbosity(tf.logging.INFO)\n",
        "logging.set_verbosity(logging.INFO)\n",
        "# get TF logger\n",
        "logger = logging.getLogger(\"tensorflow\")\n",
        "logger.addHandler(fh)\n",
        "runners.run_train(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkUzDDFAiPx2"
      },
      "source": [
        "#### Step 2: Running task-specific submodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLsCriv_iPx2"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "\n",
        "with open(config.testset_path, \"rb\") as f:\n",
        "    Vs_test = pickle.load(f)\n",
        "dataset_size = len(Vs_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0PBExAMiPx2"
      },
      "outputs": [],
      "source": [
        "step = None\n",
        "\n",
        "tf.Graph().as_default()\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "inputs, targets, mmsis, time_starts, time_ends, lengths, model = (\n",
        "    runners.create_dataset_and_model(config, shuffle=False, repeat=False)\n",
        ")\n",
        "\n",
        "if config.mode == \"traj_reconstruction\":\n",
        "    config.missing_data = True\n",
        "\n",
        "track_sample, track_true, log_weights, ll_per_t, ll_acc, _, _, _ = (\n",
        "    runners.create_eval_graph(inputs, targets, lengths, model, config)\n",
        ")\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.train.SingularMonitoredSession()\n",
        "runners.wait_for_checkpoint(saver, sess, config.logdir)\n",
        "step = sess.run(global_step)\n",
        "\n",
        "if step is None:\n",
        "    # The log filename contains the step.\n",
        "    index_filename = sorted(glob.glob(config.logdir+\"/*.index\"))[-1] # the lastest step\n",
        "    step = int(index_filename.split(\".index\")[0].split(\"ckpt-\")[-1])\n",
        "\n",
        "\n",
        "print(\"Global step: \", step)\n",
        "outputs_path = \"results/\"\\\n",
        "            + config.trainingset_path.split(\"/\")[-2] + \"/\"\\\n",
        "            + \"logprob-\"\\\n",
        "            + os.path.basename(config.trainingset_name) + \"-\"\\\n",
        "            + os.path.basename(config.testset_name) + \"-\"\\\n",
        "            + str(config.latent_size)\\\n",
        "            + \"-missing_data-\" + str(config.missing_data)\\\n",
        "            + \"-step-\"+str(step)\\\n",
        "            + \".pkl\"\n",
        "if not os.path.exists(os.path.dirname(outputs_path)):\n",
        "    os.makedirs(os.path.dirname(outputs_path))\n",
        "\n",
        "save_dir = \"results/\"\\\n",
        "            + config.trainingset_path.split(\"/\")[-2] + \"/\"\\\n",
        "            + \"local_logprob-\"\\\n",
        "            + os.path.basename(config.trainingset_name) + \"-\"\\\n",
        "            + os.path.basename(config.testset_name).replace(\"test\",\"valid\") + \"-\"\\\n",
        "            + str(config.latent_size) + \"-\"\\\n",
        "            + \"missing_data-\" + str(config.missing_data)\\\n",
        "            + \"-step-\"+str(step)\\\n",
        "            +\"/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MpOsJPriPx2"
      },
      "source": [
        "##### save_logprob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx2DvKrziPx2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "FIG_DPI = 150\n",
        "\n",
        "l_dict = []\n",
        "for d_i in tqdm(list(range(math.ceil(dataset_size / config.batch_size)))):\n",
        "    inp, tar, mmsi, t_start, t_end, seq_len, log_weights_np, true_np, ll_t = (\n",
        "        sess.run(\n",
        "            [\n",
        "                inputs,\n",
        "                targets,\n",
        "                mmsis,\n",
        "                time_starts,\n",
        "                time_ends,\n",
        "                lengths,\n",
        "                log_weights,\n",
        "                track_true,\n",
        "                ll_per_t,\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    for d_idx_inbatch in range(inp.shape[1]):\n",
        "        D = dict()\n",
        "        seq_len_d = seq_len[d_idx_inbatch]\n",
        "        D[\"seq\"] = np.nonzero(tar[:seq_len_d, d_idx_inbatch, :])[1].reshape(-1, 4)\n",
        "        D[\"t_start\"] = t_start[d_idx_inbatch]\n",
        "        D[\"t_end\"] = t_end[d_idx_inbatch]\n",
        "        D[\"mmsi\"] = mmsi[d_idx_inbatch]\n",
        "        D[\"log_weights\"] = log_weights_np[:seq_len_d, :, d_idx_inbatch]\n",
        "        l_dict.append(D)\n",
        "with open(outputs_path, \"wb\") as f:\n",
        "    pickle.dump(l_dict, f)\n",
        "\n",
        "v_logprob = np.empty((0,))\n",
        "v_logprob_stable = np.empty((0,))\n",
        "\n",
        "count = 0\n",
        "for D in tqdm(l_dict):\n",
        "    log_weights_np = D[\"log_weights\"]\n",
        "    ll_t = np.mean(log_weights_np)\n",
        "    v_logprob = np.concatenate((v_logprob, [ll_t]))\n",
        "\n",
        "d_mean = np.mean(v_logprob)\n",
        "d_std = np.std(v_logprob)\n",
        "d_thresh = d_mean - 3 * d_std\n",
        "\n",
        "plt.figure(figsize=(1920/FIG_DPI, 640/FIG_DPI), dpi=FIG_DPI)\n",
        "plt.plot(v_logprob,'o')\n",
        "plt.title(\"Log likelihood \" + os.path.basename(config.testset_name)\\\n",
        "            + \", mean = {0:02f}, std = {1:02f}, threshold = {2:02f}\".format(d_mean, d_std, d_thresh))\n",
        "plt.plot([0,len(v_logprob)], [d_thresh, d_thresh],'r')\n",
        "\n",
        "plt.xlim([0,len(v_logprob)])\n",
        "fig_name = \"results/\"\\\n",
        "        + config.trainingset_path.split(\"/\")[-2] + \"/\" \\\n",
        "        + \"logprob-\" \\\n",
        "        + config.bound + \"-\"\\\n",
        "        + os.path.basename(config.trainingset_name) + \"-\"\\\n",
        "        + os.path.basename(config.testset_name)\\\n",
        "        + \"-latent_size-\" + str(config.latent_size)\\\n",
        "        + \"-ll_thresh\" + str(round(d_thresh, 2))\\\n",
        "        + \"-missing_data-\" + str(config.missing_data)\\\n",
        "        + \"-step-\"+str(step)\\\n",
        "        + \".png\"\n",
        "plt.savefig(fig_name,dpi = FIG_DPI)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfgePSoEiPx2"
      },
      "source": [
        "![](https://github.com/movingpandas/mobiml/blob/main/examples/fig_name?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xinacvT_iPx3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(filename=fig_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzJm_jRziPx3"
      },
      "source": [
        "##### local_logprob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwSY5zofiPx3"
      },
      "outputs": [],
      "source": [
        "import mobiml.models.geotracknet.utils as utils\n",
        "\n",
        "LOGPROB_MEAN_MIN = -10.0\n",
        "LOGPROB_STD_MAX = 5\n",
        "\n",
        "LAT_RANGE = config.lat_max - config.lat_min\n",
        "LON_RANGE = config.lon_max - config.lon_min\n",
        "FIG_W = 960\n",
        "FIG_H = int(FIG_W*LAT_RANGE/LON_RANGE)\n",
        "\n",
        "m_map_logprob_std = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
        "m_map_logprob_mean = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
        "m_map_density = np.zeros(shape=(config.n_lat_cells,config.n_lon_cells))\n",
        "v_logprob = np.empty((0,))\n",
        "v_mmsi = np.empty((0,))\n",
        "Map_logprob = dict()\n",
        "for row  in range(config.n_lat_cells):\n",
        "    for col in range(config.n_lon_cells):\n",
        "        Map_logprob[ str(str(row)+\",\"+str(col))] = []\n",
        "\n",
        "# Load logprob\n",
        "with open(outputs_path,\"rb\") as f:\n",
        "    l_dict = pickle.load(f)\n",
        "\n",
        "print(\"Calculating the logprob map...\")\n",
        "for D in tqdm(l_dict):\n",
        "    tmp = D[\"seq\"]\n",
        "    log_weights_np = D[\"log_weights\"]\n",
        "    for d_timestep in range(2*6,len(tmp)):\n",
        "        try:\n",
        "            row = int(tmp[d_timestep,0]*0.01/config.cell_lat_reso)\n",
        "            col = int((tmp[d_timestep,1]-config.onehot_lat_bins)*0.01/config.cell_lat_reso)\n",
        "            Map_logprob[str(row)+\",\"+str(col)].append(np.mean(log_weights_np[d_timestep,:]))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# Remove outliers\n",
        "for row  in range(config.n_lat_cells):\n",
        "    for col in range(config.n_lon_cells):\n",
        "        s_key = str(row)+\",\"+str(col)\n",
        "        Map_logprob[s_key] = utils.remove_gaussian_outlier(np.array(Map_logprob[s_key]))\n",
        "        m_map_logprob_mean[row,col] = np.mean(Map_logprob[s_key])\n",
        "        m_map_logprob_std[row,col] = np.std(Map_logprob[s_key])\n",
        "        m_map_density[row,col] = len(Map_logprob[s_key])\n",
        "\n",
        "# Save to disk\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "np.save(save_dir+\"map_density-\"+str(config.cell_lat_reso)+\"-\"+str(config.cell_lon_reso),m_map_density)\n",
        "with open(os.path.join(save_dir,\"Map_logprob-\"+str(config.cell_lat_reso)+\"-\"+str(config.cell_lon_reso)+\".pkl\"),\"wb\") as f:\n",
        "    pickle.dump(Map_logprob,f)\n",
        "\n",
        "# Show the map\n",
        "utils.show_logprob_map(m_map_logprob_mean, m_map_logprob_std, save_dir,\n",
        "                        logprob_mean_min = LOGPROB_MEAN_MIN,\n",
        "                        logprob_std_max = LOGPROB_STD_MAX,\n",
        "                        fig_w = FIG_W, fig_h = FIG_H,\n",
        "                        )\n",
        "\n",
        "print(f'Maps stored saved to: {os.path.join(save_dir, \"logprob_std_map.png\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnYF3x7ziPx3"
      },
      "outputs": [],
      "source": [
        "Image(filename=os.path.join(save_dir, \"logprob_std_map.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qKgyIA3iPx3"
      },
      "source": [
        "##### contrario_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms4nd5e7iPx3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "import mobiml.models.geotracknet.contrario_utils as contrario_utils\n",
        "\n",
        "with open(os.path.join(save_dir,\"Map_logprob-\"+\\\n",
        "            str(config.cell_lat_reso)+\"-\"+str(config.cell_lat_reso)+\".pkl\"),\"rb\") as f:\n",
        "    Map_logprob = pickle.load(f)\n",
        "# Load the logprob\n",
        "with open(outputs_path,\"rb\") as f:\n",
        "    l_dict = pickle.load(f)\n",
        "d_i = 0\n",
        "v_mean_log = []\n",
        "l_v_A = []\n",
        "v_buffer_count = []\n",
        "length_track = len(l_dict[0][\"seq\"])\n",
        "l_dict_anomaly = []\n",
        "n_error = 0\n",
        "for D in tqdm(l_dict):\n",
        "    try:\n",
        "    # if True:\n",
        "        tmp = D[\"seq\"]\n",
        "        m_log_weights_np = D[\"log_weights\"]\n",
        "        v_A = np.zeros(len(tmp))\n",
        "        for d_timestep in range(2*6,len(tmp)):\n",
        "            d_row = int(tmp[d_timestep,0]*config.onehot_lat_reso/config.cell_lat_reso)\n",
        "            d_col = int((tmp[d_timestep,1]-config.onehot_lat_bins)*config.onehot_lat_reso/config.cell_lon_reso)\n",
        "            d_logprob_t = np.mean(m_log_weights_np[d_timestep,:])\n",
        "\n",
        "            # KDE\n",
        "            l_local_log_prod = Map_logprob[str(d_row)+\",\"+str(d_col)]\n",
        "            if len(l_local_log_prod) < 2:\n",
        "                v_A[d_timestep] = 2\n",
        "            else:\n",
        "                kernel = stats.gaussian_kde(l_local_log_prod)\n",
        "                cdf = kernel.integrate_box_1d(-np.inf,d_logprob_t)\n",
        "                if cdf < 0.1:\n",
        "                    v_A[d_timestep] = 1\n",
        "        v_A = v_A[12:]\n",
        "        v_anomalies = np.zeros(len(v_A))\n",
        "        for d_i_4h in range(0,len(v_A)+1-24):\n",
        "            v_A_4h = v_A[d_i_4h:d_i_4h+24]\n",
        "            v_anomalies_i = contrario_utils.contrario_detection(v_A_4h,config.contrario_eps)\n",
        "            v_anomalies[d_i_4h:d_i_4h+24][v_anomalies_i==1] = 1\n",
        "\n",
        "        if len(contrario_utils.nonzero_segments(v_anomalies)) > 0:\n",
        "            D[\"anomaly_idx\"] = v_anomalies\n",
        "            l_dict_anomaly.append(D)\n",
        "    except:\n",
        "        n_error += 1\n",
        "print(\"Number of processed tracks: \",len(l_dict))\n",
        "print(\"Number of abnormal tracks: \",len(l_dict_anomaly))\n",
        "print(\"Number of errors: \",n_error)\n",
        "\n",
        "# Save to disk\n",
        "n_anomalies = len(l_dict_anomaly)\n",
        "save_filename = os.path.basename(config.trainingset_name)\\\n",
        "                +\"-\" + os.path.basename(config.trainingset_name)\\\n",
        "                +\"-\" + str(config.latent_size)\\\n",
        "                +\"-missing_data-\"+str(config.missing_data)\\\n",
        "                +\"-step-\"+str(step)\\\n",
        "                +\".pkl\"\n",
        "save_pkl_filename = os.path.join(save_dir,\"List_abnormal_tracks-\"+save_filename)\n",
        "with open(save_pkl_filename,\"wb\") as f:\n",
        "    pickle.dump(l_dict_anomaly,f)\n",
        "\n",
        "## Plot\n",
        "with open(config.trainingset_path,\"rb\") as f:\n",
        "    Vs_train = pickle.load(f)\n",
        "with open(config.testset_path,\"rb\") as f:\n",
        "    Vs_test = pickle.load(f)\n",
        "\n",
        "save_filename = \"Abnormal_tracks\"\\\n",
        "            + \"-\" + os.path.basename(config.trainingset_name)\\\n",
        "            + \"-\" + os.path.basename(config.testset_name)\\\n",
        "            + \"-latent_size-\" + str(config.latent_size)\\\n",
        "            + \"-step-\"+str(step)\\\n",
        "            + \"-eps-\"+str(config.contrario_eps)\\\n",
        "            + \"-\" + str(n_anomalies)\\\n",
        "            + \".png\"\n",
        "\n",
        "# Plot abnormal tracks with the tracks in the training set as the background\n",
        "utils.plot_abnormal_tracks(Vs_train,l_dict_anomaly,\n",
        "                    os.path.join(save_dir,save_filename),\n",
        "                    config.lat_min,config.lat_max,config.lon_min,config.lon_max,\n",
        "                    config.onehot_lat_bins,config.onehot_lon_bins,\n",
        "                    background_cmap = \"Blues\",\n",
        "                    fig_w = FIG_W, fig_h = FIG_H,\n",
        "                )\n",
        "plt.close()\n",
        "# Plot abnormal tracks with the tracks in the test set as the background\n",
        "utils.plot_abnormal_tracks(Vs_test,l_dict_anomaly,\n",
        "                    os.path.join(save_dir,save_filename.replace(\"Abnormal_tracks\",\"Abnormal_tracks2\")),\n",
        "                    config.lat_min,config.lat_max,config.lon_min,config.lon_max,\n",
        "                    config.onehot_lat_bins,config.onehot_lon_bins,\n",
        "                    background_cmap = \"Greens\",\n",
        "                    fig_w = FIG_W, fig_h = FIG_H,\n",
        "                )\n",
        "plt.close()\n",
        "# Save abnormal tracks to csv file\n",
        "with open(os.path.join(save_dir,save_filename.replace(\".png\",\".csv\")),\"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"MMSI\",\"Time_start\",\"Time_end\",\"Timestamp_start\",\"Timestamp_end\"])\n",
        "    for D in l_dict_anomaly:\n",
        "        writer.writerow([D[\"mmsi\"],\n",
        "            datetime.utcfromtimestamp(D[\"t_start\"]).strftime('%Y-%m-%d %H:%M:%SZ'),\n",
        "            datetime.utcfromtimestamp(D[\"t_end\"]).strftime('%Y-%m-%d %H:%M:%SZ'),\n",
        "            D[\"t_start\"],D[\"t_end\"]])\n",
        "\n",
        "print(f'Maps stored saved to: {save_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF5uitgNiPx3"
      },
      "outputs": [],
      "source": [
        "Image(filename=os.path.join(save_dir,save_filename.replace(\"Abnormal_tracks\",\"Abnormal_tracks2\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsEK2gkTiPx3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}